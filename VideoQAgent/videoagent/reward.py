# Copyright 2025 [Your Name or Organization]
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import re
import torch
from typing import Any, Tuple, Dict
from omegaconf import DictConfig
import ray
from verl import DataProto


def compute_tvqa_score(solution_str: str, ground_truth: str, format_score: float = 0.0, one_turn: bool = True) -> Tuple[float, float]:
    """
    è®¡ç®— TVQA+ ä»»åŠ¡çš„åˆ†æ•°ã€‚

    Args:
        solution_str: æ¨¡å‹ç”Ÿæˆçš„å®Œæ•´å›ç­”ï¼ˆåŒ…å« promptï¼‰
        ground_truth: æ­£ç¡®ç­”æ¡ˆï¼ˆå¦‚ "a1"ï¼‰
        format_score: æ ¼å¼åˆ†æ•°ï¼ˆå½“æ¨¡å‹æ²¡æœ‰æŒ‰æ ¼å¼å›ç­”æ—¶ç»™äºˆçš„åˆ†æ•°ï¼Œé»˜è®¤ä¸º 0.0ï¼‰
        one_turn: æ˜¯å¦ä¸ºä¸€è½®é—®é¢˜ï¼ˆTrue ä¸º T=1 ç±»ï¼ŒFalse ä¸º T>1 ç±»ï¼‰

    Returns:
        Tuple[float, float]: ä¸»åˆ†æ•°å’Œç¬¬äºŒä¸ªåˆ†æ•°ï¼ŒåŸºäºä»¥ä¸‹è§„åˆ™ï¼š
            - <reasoning>...</reasoning> æ ‡ç­¾å­˜åœ¨ï¼š+0.2
            - <answer>...</answer> æ ‡ç­¾å­˜åœ¨ï¼š+0.2
            - ç­”æ¡ˆæ­£ç¡®ï¼š+1.0
            - T>1 æ—¶ï¼Œå¦‚æœç­”æ¡ˆé”™è¯¯ä¸”ä½¿ç”¨äº†å·¥å…·ï¼ˆ<search> æˆ– <request_grounding>ï¼‰ï¼š+0.5
    """

    response = solution_str
    total_score = 0.0
    second_score = 0.0  # åˆå§‹åŒ–ç¬¬äºŒä¸ªåˆ†æ•°
    reasoning_match = re.search(r"<reasoning>.*?</reasoning>", response, re.DOTALL)
    if reasoning_match:
        total_score += 0.2

    answer_match = re.search(r"<answer>(.*?)</answer>", response, re.DOTALL)
    if not answer_match:
        return total_score, second_score

    total_score += 0.2
    predicted_answer = answer_match.group(1).strip()

    predicted_num_match = re.search(r'(\d+)', predicted_answer)
    ground_truth_num_match = re.search(r'(\d+)', ground_truth)

    predicted_num = predicted_num_match.group(1) if predicted_num_match else None
    ground_truth_num = ground_truth_num_match.group(1) if ground_truth_num_match else None

    answer_correct = False
    if predicted_num is not None and ground_truth_num is not None:
        if predicted_num == ground_truth_num:
            total_score += 1.0
            answer_correct = True
    else:
        print(f"Warning: Unable to extract numbers for comparison - predicted: {predicted_answer}, ground_truth: {ground_truth}")

    if not one_turn:
        used_tool = re.search(r"<search>", response) or re.search(r"<request_grounding>", response)
        if used_tool:
            second_score += 1  # ä¿®å¤å¹¶æ·»åŠ  +1 åˆ°ç¬¬äºŒä¸ªåˆ†æ•°

    return total_score, second_score


def _select_rm_score_fn(data_source: str) -> callable:
    """
    æ ¹æ®æ•°æ®æºé€‰æ‹©åˆé€‚çš„å¥–åŠ±å‡½æ•°ã€‚

    Args:
        data_source: æ•°æ®æºåç§°ã€‚

    Returns:
        callable: å¯¹åº”çš„å¥–åŠ±å‡½æ•°ï¼Œè¿”å› Tuple[float, float]ã€‚
    """
    if data_source == 'tvqa_plus_vision':
        return compute_tvqa_score
    elif data_source in ['nq', 'triviaqa', 'popqa', 'hotpotqa', '2wikimultihopqa', 'musique', 'bamboogle']:
        from verl.utils.reward_score import qa_em
        def wrapped_fn(solution_str, ground_truth, format_score=0.0, one_turn=True):
            main_score = qa_em.compute_score_em(solution_str, ground_truth, format_score, one_turn)
            second_score = 1.0 if solution_str.strip() == ground_truth.strip() else 0.0  # ç¤ºä¾‹ï¼šç®€å• exact match ä½œä¸ºç¬¬äºŒä¸ªåˆ†æ•°
            return main_score, second_score
        return wrapped_fn
    else:
        print(f"Warning: Unknown data source '{data_source}', using default score 0.0")
        return lambda solution_str, ground_truth, format_score=0.0, one_turn=True: (0.0, 0.0)


class CustomRewardManager:
    """è‡ªå®šä¹‰å¥–åŠ±ç®¡ç†å™¨ï¼Œå®Œå…¨å¤åˆ¶æ—§ç‰ˆæœ¬é€»è¾‘ã€‚"""

    def __init__(self, tokenizer, num_examine, format_score=0.0, **kwargs) -> None:
        self.tokenizer = tokenizer
        self.num_examine = num_examine
        self.format_score = format_score

    def __call__(self, data: DataProto, return_dict=False, **kwargs):
        if 'rm_scores' in data.batch.keys():
            if return_dict:
                return {
                    'reward_tensor': data.batch['rm_scores'],
                    'reward_extra_info': {}
                }
            return data.batch['rm_scores']

        reward_tensor = torch.zeros_like(data.batch['responses'], dtype=torch.float32)
        all_main_scores = []
        all_second_scores = []
        already_print_data_sources = {}

        for i in range(len(data)):
            data_item = data[i]

            prompt_ids = data_item.batch['prompts']

            prompt_length = prompt_ids.shape[-1]
            valid_prompt_length = data_item.batch['attention_mask'][:prompt_length].sum()
            valid_prompt_ids = prompt_ids[:valid_prompt_length]

            response_ids = data_item.batch['responses']
            response_mask = data_item.batch['attention_mask'][prompt_length:]
            valid_response_ids = response_ids[response_mask.bool()]  # â† å…³é”®ä¿®å¤
            valid_response_length = valid_response_ids.shape[0]       # ä»ç­‰äº sum

            # sequences_str = self.tokenizer.decode(torch.cat((prompt_ids, response_ids)), skip_special_tokens=True)  # full decode for compare 
            sequences = torch.cat((valid_prompt_ids, valid_response_ids))
            sequences_str = self.tokenizer.decode(sequences, skip_special_tokens=True)



            ground_truth = data_item.non_tensor_batch['reward_model']['ground_truth']
            one_turn = data_item.non_tensor_batch['reward_model'].get('one_turn', True)

            data_source = data_item.non_tensor_batch['data_source']
            compute_score_fn = _select_rm_score_fn(data_source)

            score_tuple = compute_score_fn(
                solution_str=sequences_str,
                ground_truth=ground_truth,
                format_score=self.format_score,
                one_turn=one_turn
            )
            main_score, second_score = score_tuple if isinstance(score_tuple, tuple) else (score_tuple, 0.0)

            reward_tensor[i, valid_response_length - 1] = main_score
            all_main_scores.append(main_score)
            all_second_scores.append(second_score)

            if data_source not in already_print_data_sources:
                already_print_data_sources[data_source] = 0

            if already_print_data_sources[data_source] < self.num_examine:
                already_print_data_sources[data_source] += 1
                if data_source == 'tvqa_plus_vision':
                    print(f"\nğŸ” Sample {i} (Data source: {data_source}):")
                    print(f"Ground truth: {ground_truth}")
                    print(f"Response: {sequences_str}")
                    print(f"Main Score: {main_score}, Second Score: {second_score}")
                else:
                    print(f"Sample {i} (Data source: {data_source}): {sequences_str}")

        reward_extra_info = {'all_main_scores': all_main_scores, 'all_second_scores': all_second_scores}

        if return_dict:
            return {
                'reward_tensor': reward_tensor,
                'reward_extra_info': reward_extra_info
            }
        return reward_tensor


def load_reward_manager(config: DictConfig, tokenizer: Any, num_examine: int, **reward_kwargs: Any) -> Any:
    """
    åŠ è½½è‡ªå®šä¹‰å¥–åŠ±ç®¡ç†å™¨ã€‚

    Args:
        config: PPO è®­ç»ƒå™¨é…ç½®å¯¹è±¡ã€‚
        tokenizer: åˆ†è¯å™¨å¯¹è±¡ã€‚
        num_examine: æ‰“å°è°ƒè¯•ä¿¡æ¯çš„æ ·æœ¬æ•°é‡ã€‚
        **reward_kwargs: å¥–åŠ±ç®¡ç†å™¨çš„é¢å¤–å‚æ•°ã€‚

    Returns:
        CustomRewardManager: è‡ªå®šä¹‰å¥–åŠ±ç®¡ç†å™¨å®ä¾‹ã€‚
    """
    print("âœ… Loading CustomRewardManager for TVQA+ support")
    return CustomRewardManager(
        tokenizer=tokenizer,
        num_examine=num_examine,
        format_score=reward_kwargs.get('format_score', 0.0),
        **reward_kwargs
    )


def compute_reward(data: DataProto, reward_fn: Any) -> Tuple[torch.Tensor, Dict[str, Any]]:
    """
    ä¸ºä¸€æ‰¹æ•°æ®è®¡ç®—å¥–åŠ±ã€‚

    Args:
        data: DataProto å¯¹è±¡ã€‚
        reward_fn: å¥–åŠ±å‡½æ•°ï¼ˆCustomRewardManager å®ä¾‹ï¼‰ã€‚

    Returns:
        Tuple[torch.Tensor, Dict[str, Any]]: å¥–åŠ±å¼ é‡å’Œé¢å¤–ä¿¡æ¯å­—å…¸ã€‚
    """
    try:
        reward_result = reward_fn(data, return_dict=True)
        reward_tensor = reward_result['reward_tensor']
        reward_extra_info = reward_result.get('reward_extra_info', {})
    except Exception as e:
        print(f"Error in reward_fn: {e}")
        reward_tensor = reward_fn(data)
        reward_extra_info = {}

    return reward_tensor, reward_extra_info


@ray.remote(num_cpus=1)
def compute_reward_async(data: DataProto, config=None, tokenizer=None, reward_fn=None):
    """
    å¼‚æ­¥è®¡ç®—ä¸€æ‰¹æ•°æ®çš„å¥–åŠ±ã€‚

    Args:
        data: DataProto å¯¹è±¡ã€‚
        config: é…ç½®å¯¹è±¡ï¼ˆå¯é€‰ï¼‰ã€‚
        tokenizer: åˆ†è¯å™¨å¯¹è±¡ï¼ˆå¯é€‰ï¼‰ã€‚
        reward_fn: å¥–åŠ±å‡½æ•°ï¼ˆCustomRewardManager å®ä¾‹ï¼‰ã€‚

    Returns:
        Tuple[torch.Tensor, Dict[str, Any]]: å¥–åŠ±å¼ é‡å’Œé¢å¤–ä¿¡æ¯å­—å…¸ã€‚
    """
    if reward_fn is None:
        assert config is not None and tokenizer is not None
        reward_fn = load_reward_manager(
            config, tokenizer, num_examine=0, **config.reward_model.get("reward_kwargs", {})
        )

    return compute_reward(data, reward_fn)