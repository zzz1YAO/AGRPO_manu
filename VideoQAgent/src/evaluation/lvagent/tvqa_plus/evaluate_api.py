from pathlib import Path
from PIL import Image
import io, base64, math, re
from openai import OpenAI
import os
import os, re, json, argparse, time
from typing import Dict, List, Any, Tuple
from openai import OpenAI
import random
import re
import concurrent.futures  # æ–°å¢ï¼šç”¨äºå¤šçº¿ç¨‹

def bbox_to_string_simplified(file_path="../Tvqa_data/clip_bbox_mapping.json", key=None):
    """
    æ ¹æ®ç»™å®šçš„ key ä» JSON æ–‡ä»¶ä¸­æå– BBOX ä¿¡æ¯ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºç®€åŒ–çš„å­—ç¬¦ä¸²æ ¼å¼ï¼Œå¸§æŒ‰ frame_id æ’åºã€‚
    
    å‚æ•°:
        file_path (str): JSON æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º '../Tvqa_data/clip_bbox_mapping.json'
        key (str): è¦æŸ¥è¯¢çš„ keyï¼Œä¾‹å¦‚ "s10e03_seg02_clip_07"
    
    è¿”å›:
        str: ç®€åŒ–çš„ BBOX ä¿¡æ¯å­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸º "Frame {frame_id}: - {name}: ({x}, {y}, {width}, {height})"
             å¦‚æœ key æˆ–æ–‡ä»¶è·¯å¾„æ— æ•ˆï¼Œåˆ™è¿”å›é”™è¯¯ä¿¡æ¯
    """
    try:
        # è¯»å– JSON æ–‡ä»¶
        with open(file_path, 'r') as f:
            json_data = json.load(f)
        
        # æ£€æŸ¥ key æ˜¯å¦å­˜åœ¨äº JSON æ•°æ®ä¸­
        if key not in json_data:
            return f"Error: Key '{key}' not found in JSON data."
        
        # è·å–æŒ‡å®š key çš„ BBOX æ•°æ®
        bbox_data = json_data[key]
        
        # åˆå§‹åŒ–ç»“æœå­—ç¬¦ä¸²
        result = []
        
        # å¯¹ frame_id è¿›è¡Œæ’åºï¼ˆå‡è®¾ frame_id æ˜¯å­—ç¬¦ä¸²å½¢å¼çš„æ•°å­—ï¼‰
        sorted_frame_ids = sorted(bbox_data.keys(), key=lambda x: int(x) if x.isdigit() else x)
        
        # éå†æ’åºåçš„ frame_id
        for frame_id in sorted_frame_ids:
            result.append(f"Frame {frame_id}:")
            # éå†è¯¥å¸§ä¸­çš„æ¯ä¸ª BBOX
            for bbox in bbox_data[frame_id]:
                x, y, width, height, name = bbox
                result.append(f"  - {name}: ({x}, {y}, {width}, {height})")
        
        # å°†ç»“æœåˆå¹¶ä¸ºå­—ç¬¦ä¸²
        return "\n".join(result)
    
    except FileNotFoundError:
        return f"Error: File '{file_path}' not found."
    except json.JSONDecodeError:
        return f"Error: Invalid JSON format in file '{file_path}'."
    except Exception as e:
        return f"Error processing JSON data: {str(e)}"

def convert_image_to_base64_data_url(path: str) -> str | None:
    try:
        with Image.open(path) as img:
            if img.mode in ('RGBA', 'P'):
                img = img.convert('RGB')
            buf = io.BytesIO(); img.save(buf, format='JPEG')
            return "data:image/jpeg;base64," + base64.b64encode(buf.getvalue()).decode()
    except Exception:
        return None

def process_and_query_seg(
    seg: dict,
    vid: str,
    text_client: OpenAI,
    vision_client: OpenAI,
    base_frame_dir: str = "../Tvqa_data/bbt_frames",
    model: str = "qwen3-vl-235b-a22b-instruct"
) -> str:
    messages_content = []
    # Modified: No longer sample based on time_str; instead, fixed sampling from clip: frames 1 to 180, every 10 frames
    frame_nums = list(range(1, 181, 12))  # Results in [1, 11, 21, ..., 171], approximately 18 frames

    print(f"  Selected {len(frame_nums)} frames for vision query: {frame_nums}")
    for fn in frame_nums:
        img_path = Path(base_frame_dir, vid, f"{fn:05d}.jpg")
        if img_path.is_file():
            url = convert_image_to_base64_data_url(str(img_path))
            if url:
                messages_content.append({
                    "type": "image_url",
                    "image_url": {"url": url}
                })
    bbox_info=bbox_to_string_simplified(key=vid)
    messages_content.append({
        "type": "text",
        "text": (
            f"Images 1-{len(frame_nums)} are video frames extracted from frames 1 to 180. Bounding box information is provided in {bbox_info}. You can focus on the key objects and actions within these bounding boxes in the frames.\nAnd here is a description of what I want to know:\n"
            f"{seg['description']}"
        )
    })

    resp = vision_client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": messages_content}]
    )
    return resp.choices[0].message.content
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ç®€åŒ–ç‰ˆactionè§£æ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_action_from_response(response: str) -> Tuple[str, str]:
    """
    ä»å“åº”ä¸­è§£æactionä¿¡æ¯ - ä¿®æ”¹ç‰ˆï¼Œæ”¯æŒä¸‰ä¸ªactionï¼Œæ— time_range
    
    Returns:
        (action_type, content)
        action_type: 'search', 'request_grounding', 'answer', 'invalid'
    """
    # æ£€æŸ¥searchï¼ˆä½¿ç”¨re.searchä»å¤´åŒ¹é…ç¬¬ä¸€ä¸ªå‡ºç°çš„æ ‡ç­¾ï¼‰
    search_match = re.search(r'<search>(.*?)</search>', response, re.DOTALL)
    if search_match:
        content = search_match.group(1).strip()
        # æˆªæ–­responseåˆ°è¿™ä¸ªæ ‡ç­¾ç»“æŸçš„ä½ç½®
        end_pos = search_match.end()
        response = response[:end_pos]  # å®ç°åæˆªæ–­ï¼šåªä¿ç•™åˆ°ç¬¬ä¸€ä¸ªæ ‡ç­¾ç»“æŸ
        return 'search', content
    
    # æ£€æŸ¥request_grounding
    grounding_match = re.search(r'<request_grounding>(.*?)</request_grounding>', response, re.DOTALL)
    if grounding_match:
        content = grounding_match.group(1).strip()
        # æˆªæ–­responseåˆ°è¿™ä¸ªæ ‡ç­¾ç»“æŸçš„ä½ç½®
        end_pos = grounding_match.end()
        response = response[:end_pos]
        return 'request_grounding', content
    
    # æ£€æŸ¥answer
    answer_match = re.search(r'<answer>(.*?)</answer>', response, re.DOTALL)
    if answer_match:
        content = answer_match.group(1).strip()
        # æˆªæ–­responseåˆ°è¿™ä¸ªæ ‡ç­¾ç»“æŸçš„ä½ç½®
        end_pos = answer_match.end()
        response = response[:end_pos]
        return 'answer', content
    
    # æ— æœ‰æ•ˆaction
    return 'invalid', ''

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ‰§è¡ŒActionçš„å¤„ç†å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def execute_action(action_type: str, content: str, 
                  vid: str, text_client: OpenAI, vision_client: OpenAI,
                  question_data: Dict, episode_sub_block: str, clip_subtitles: Dict[str, str],
                  bbox_data: Dict = None) -> Tuple[str, bool]:
    """
    æ‰§è¡Œå…·ä½“çš„action - ä¿®æ”¹ç‰ˆï¼Œæ— time_rangeï¼Œæ”¯æŒrequest_groundingï¼Œä»…è§†è§‰æŸ¥è¯¢
    
    Args:
        action_type: 'answer', 'search', 'request_grounding', 'invalid'
        content: actionå†…å®¹
        vid: è§†é¢‘ID
        text_client: æ–‡æœ¬APIå®¢æˆ·ç«¯
        vision_client: è§†è§‰APIå®¢æˆ·ç«¯
        question_data: é—®é¢˜æ•°æ®å­—å…¸ï¼ŒåŒ…å«q, a0-a4ç­‰ï¼ˆç”¨äºgroundingï¼‰
        episode_sub_block: å‰§é›†çº§åˆ«çš„å­—å¹•å—ï¼ˆç”¨äºgroundingï¼‰
        clip_subtitles: clipçº§åˆ«çš„å­—å¹•å­—å…¸ï¼ˆç”¨äºè·å–æ–°subï¼‰
        bbox_data: bboxæ•°æ®å­—å…¸ï¼ˆå¯é€‰ï¼Œä½†æœ¬ç‰ˆæœ¬ä¸ä½¿ç”¨ï¼‰
        
    Returns:
        (result_content, is_done) - å¦‚æœæ˜¯answeråˆ™è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    if action_type == 'answer':
        return f"\n<answer>{content}</answer>", True  # è¿”å›ç­”æ¡ˆå¹¶ç»“æŸ
    
    elif action_type == 'search':
        try:
            # ğŸ”¥ åªæ‰§è¡Œè§†è§‰æŸ¥è¯¢ï¼ˆç§»é™¤æ—¶é—´ä¾èµ–å’ŒbboxæŸ¥è¯¢ï¼‰

            seg = {
                "description": content  # queryä½œä¸ºdescription
            }
            
            information_parts = []
            
            # åŸæœ‰çš„è§†è§‰LLMæŸ¥è¯¢
            try:
                vision_response = process_and_query_seg(seg, vid, text_client, grounding_client,model="gpt-4o")
                information_parts.append(f"Visual Description:\n{vision_response.strip()}")
                
            except Exception as e:
                print(f"Vision LLM call failed: {e}")
                information_parts.append(f"Visual Description: Error - {str(e)}")
            
            bbox_info=bbox_to_string_simplified(key=vid)
            
            # 3. åˆå¹¶æ‰€æœ‰ä¿¡æ¯ï¼ˆä»…è§†è§‰ï¼‰
            combined_info = "\n".join(information_parts)
            return f'\n<information>Bounding Box:{bbox_info}\n{combined_info}</information>\n', False
            
        except Exception as e:
            print(f"Search action failed: {e}")
            return f'\n<information>Error: Failed to get information - {str(e)}</information>\n', False
    
    elif action_type == 'request_grounding':
        try:
            # è°ƒç”¨groundingé€»è¾‘ï¼ŒåŸºäºquestion_dataå’Œepisode_sub_block
            grounding_result = re_analyze_single_question_api(question_data, episode_sub_block, vid, attempt_round=1)
            
            # è·å–predicted_clip
            if "error" in grounding_result:
                print(f"Grounding failed: {grounding_result['error']}")
                result_content = f"Grounding failed for query: {content}. Error: {grounding_result['error']}"
                return f'\n<grounding_info>{result_content}</grounding_info>\n', False  # ä¸ç»“æŸï¼Œç»§ç»­å¾ªç¯
            else:
                predicted_clip = grounding_result.get("predicted_clip", vid)  # Fallback to current vid
                # ä½¿ç”¨predicted_clipç´¢å¼•sub
                new_sub = get_clip_subtitle(clip_subtitles, predicted_clip)
                # æ„å»ºè¿”å›å­—ç¬¦ä¸²
                result_content = f"<New_clip>{predicted_clip} + {new_sub}</New_clip>"
                return f'\n{result_content}\n', False  # ä¸ç»“æŸï¼Œç»§ç»­å¾ªç¯
        
        except Exception as e:
            print(f"Grounding action failed: {e}")
            return f'\n<grounding_info>Error: Failed to perform grounding - {str(e)}</grounding_info>\n', False

    else:  # invalid action
        return '\nMy action is not correct. I need to search, request grounding, or answer.\n', False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å­—å¹•å¤„ç†å‡½æ•°ï¼ˆä»grounding_clip.pyå¤åˆ¶ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_clip_subtitle(clip_subtitles: Dict[str, str], clip_name: str) -> str:
    """
    æ ¹æ®clipåç§°è·å–å¯¹åº”çš„å•ä¸ªclipå­—å¹•ã€‚
    
    Args:
        clip_subtitles: åŒ…å«æ‰€æœ‰clipå­—å¹•çš„å­—å…¸ï¼Œé”®ä¸ºclipåç§°ï¼Œå€¼ä¸ºå­—å¹•æ–‡æœ¬
        clip_name: æŒ‡å®šçš„clipåç§°ï¼ˆå¦‚ "s01e02_seg01_clip_01"ï¼‰
    
    Returns:
        æ ¼å¼åŒ–çš„å­—å¹•å­—ç¬¦ä¸²ï¼Œå½¢å¦‚ "<clip_name>subtitle_text</clip_name>"ï¼›
        å¦‚æœæœªæ‰¾åˆ°ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
    """
    subtitle_text = clip_subtitles.get(clip_name, "")
    if subtitle_text:
        return f"<{clip_name}>{subtitle_text}</{clip_name}>"
    print(f"Warning: No subtitle found for clip {clip_name}")
    return ""

def build_subtitles_for_episode(clip_subtitles: Dict[str, str], episode_prefix: str) -> str:
    """ä¸ºæŒ‡å®šå‰§é›†æ„å»ºæ‹¼æ¥åçš„å­—å¹•ï¼ŒæŒ‰clipé¡ºåºæ‹¼æ¥"""
    # æ‰¾åˆ°æ‰€æœ‰åŒ¹é…è¯¥å‰§é›†çš„clips
    matching_clips = {k: v for k, v in clip_subtitles.items() if k.startswith(episode_prefix)}
    
    # æŒ‰keyæ’åºï¼ˆç¡®ä¿é¡ºåºï¼‰
    sorted_clips = sorted(matching_clips.items())
    
    # æ‹¼æ¥æ ¼å¼åŒ–çš„å­—å¹•
    formatted_subtitles = []
    for clip_key, subtitle_text in sorted_clips:
        formatted_subtitles.append(f"<{clip_key}>{subtitle_text}</{clip_key}>")
    
    return "\n".join(formatted_subtitles)

def get_subtitles_for_video(clip_subtitles: Dict[str, str], vid_name: str) -> str:
    """æ ¹æ®è§†é¢‘åè·å–å¯¹åº”çš„å­—å¹•å—"""
    # ä»vid_nameä¸­æå–å‰§é›†å‰ç¼€ï¼Œä¾‹å¦‚ä»s01e02_abc123æå–s01e02
    episode_prefix = vid_name.split('_')[0]  # s01e02
    return build_subtitles_for_episode(clip_subtitles, episode_prefix)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ API è°ƒç”¨å‡½æ•°ï¼ˆåŒºåˆ†ä¸åŒç”¨é€”ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å…¬å…±APIå¯†é’¥å’Œbase_urlï¼ˆå‡è®¾æ‰€æœ‰APIä½¿ç”¨ç›¸åŒçš„é˜¿é‡Œäº‘DashScopeå…¼å®¹OpenAIæ ¼å¼ï¼‰
grounding_api=os.getenv("qdd_api")
vision_api=os.getenv("aliyun_api")
main_api=os.getenv("qdd_api")
tencent_api=os.getenv("tencent_api")

# å®šä½APIå®¢æˆ·ç«¯ï¼ˆç”¨äºgroundingï¼‰
grounding_client = OpenAI(
    api_key=grounding_api,
    base_url="https://api2.aigcbest.top/v1"
)

# è§†è§‰APIå®¢æˆ·ç«¯ï¼ˆç”¨äºvisionæŸ¥è¯¢ï¼‰
vision_client = OpenAI(
    api_key=vision_api,
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"  # å‡è®¾ç›¸åŒï¼Œå¦‚æœä¸åŒå¯ä¿®æ”¹
)

# ä¸»è¦pipeline APIå®¢æˆ·ç«¯ï¼ˆç”¨äºæ–‡æœ¬æ¨ç†ï¼‰
main_client = OpenAI(
    api_key=main_api,
    base_url="https://api2.aigcbest.top/v1"
)

def grounding_llm_generate(user_content: str, model: str = "grok-4-fast-reasoning") -> str:
    """
    ä½¿ç”¨APIè°ƒç”¨LLMç”Ÿæˆå“åº”ï¼ˆä¸“ç”¨äºgrounding/å®šä½ï¼‰
    """
    try:
        response = grounding_client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": user_content}],
            temperature=0.6,
            max_tokens=1024
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"Grounding API call failed: {e}")
        return "Error: Failed to generate response"

def main_llm_generate(conversation_history: str, model: str = "grok-4-fast-reasoning") -> str:
    """
    ä½¿ç”¨APIè°ƒç”¨LLMç”Ÿæˆå“åº”ï¼ˆä¸“ç”¨äºä¸»è¦pipelineçš„æ–‡æœ¬æ¨ç†ï¼‰
    """
    try:
        response = main_client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": conversation_history}],
            temperature=0.6,
            max_tokens=1024
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"Main pipeline API call failed: {e}")
        return "Error: Failed to generate response"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ åå¤„ç†å‡½æ•°ï¼ˆä»grounding_clip.pyå¤åˆ¶ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def postprocess_response(response: str) -> str:
    """å¯¹å“åº”è¿›è¡Œåå¤„ç†å’Œæˆªæ–­"""
    type_match = re.search(r'<type>.*?</type>', response, re.DOTALL)
    if type_match:
        return response[:type_match.end()]
    
    time_match = re.search(r'<time>.*?</time>', response, re.DOTALL)
    if time_match:
        return response[:time_match.end()]
    
    return response
def extract_clip_content(text: str) -> str:
    """æå–<clip></clip>æ ‡ç­¾å†…å®¹"""
    match = re.search(r"<clip>(.*?)</clip>", text, re.DOTALL)
    return match.group(1).strip() if match else ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å•æ¬¡åˆ†æå‡½æ•°ï¼ˆä»grounding_clip.pyå¤åˆ¶ï¼Œå¹¶é€‚åº”æ¨ç†é˜¶æ®µï¼šç§»é™¤GTå¯¹æ¯”ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def re_analyze_single_question_api(question_data: Dict, sub_block: str, vid: str, attempt_round: int) -> Dict:
    """
    ä½¿ç”¨APIåˆ†æå•ä¸ªé—®é¢˜çš„clipå®šä½ï¼ˆæ¨ç†é˜¶æ®µï¼šç§»é™¤GTå¯¹æ¯”ï¼‰
    """
    original_idx = question_data.get('original_idx', '?')
    print(f"      Question {original_idx}: API Analysis round {attempt_round}")
    
    # æ„å»ºpromptï¼ˆç®€åŒ–ï¼šå‡è®¾é€‰é¡¹å›ºå®šä¸ºa0-a4ï¼‰
    prompt_content = f"""
Question: {question_data['q']}
Options:
a0: {question_data.get('a0', '')}
a1: {question_data.get('a1', '')}
a2: {question_data.get('a2', '')}
a3: {question_data.get('a3', '')}
a4: {question_data.get('a4', '')}

Subtitles: {sub_block}

The subtitles are formatted as <clip_label>subtitle_content</clip_label>, where each < > pair contains a clip label followed by its corresponding subtitle content.

Based on the question and subtitles, determine:
1. The specific clip label where the answer to this question occurs or is mentioned (output in <clip>label</clip> format)
{vid} may not contain the scene or context related to the question. Please determine a different specific clip label.
Please analyze the given question and provide the following information:
<clip>
clip_label (the specific clip where the question's answer can be found in the video)
</clip>
"""
    
    try:
        raw_response = grounding_llm_generate(prompt_content)
        print(f"        Raw response: {raw_response}")
        if raw_response.startswith("Error:"):
            return {"error": raw_response}
        
        processed_response = postprocess_response(raw_response)
        predicted_clip = extract_clip_content(processed_response)
        
        print(f"        Predicted clip: {predicted_clip}")
        
        return {
            "predicted_clip": predicted_clip,
            "raw_response": raw_response,
            "processed_response": processed_response
        }
    except Exception as e:
        print(f"        âŒ Analysis error: {e}")
        return {"error": str(e)}
def extract_clip_content(text: str) -> str:
    """æå–<clip></clip>æ ‡ç­¾å†…å®¹"""
    match = re.search(r"<clip>(.*?)</clip>", text, re.DOTALL)
    return match.group(1).strip() if match else ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å•æ¬¡åˆ†æå‡½æ•°ï¼ˆä»grounding_clip.pyå¤åˆ¶ï¼Œå¹¶é€‚åº”æ¨ç†é˜¶æ®µï¼šç§»é™¤GTå¯¹æ¯”ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def analyze_single_question_api(
    question_data: Dict, 
    sub_block: str, 
    attempt_round: int,
    json_path: str = "/home/rliuay/runtao/proj_videoqa/Tvqa_data/grounding_pairs_6000_samples.json"
) -> Dict:
    """
    ä½¿ç”¨APIåˆ†æå•ä¸ªé—®é¢˜çš„clipå®šä½ï¼ˆæ¨ç†é˜¶æ®µï¼šç§»é™¤GTå¯¹æ¯”ï¼‰
    ä¿®æ”¹ï¼šå…ˆä»JSONæ–‡ä»¶ä¸­æŸ¥æ‰¾åŒ¹é…çš„questionï¼Œå¦‚æœæ‰¾åˆ°åˆ™ä½¿ç”¨å…¶clipä½œä¸ºpredicted_clipï¼›
    å¦‚æœæœªæ‰¾åˆ°ï¼Œå†è°ƒç”¨APIè¿›è¡Œgroundingã€‚
    """
    original_idx = question_data.get('original_idx', '?')
    print(f"      Question {original_idx}: API Analysis round {attempt_round}")
    
    # é¦–å…ˆå°è¯•ä»JSONæ–‡ä»¶ä¸­åŠ è½½å¹¶åŒ¹é…
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            grounding_pairs = json.load(f)
        
        target_question = question_data['q'].strip()  # ç›®æ ‡é—®é¢˜ï¼Œç§»é™¤å¯èƒ½çš„ç©ºç™½
        
        for key, entry in grounding_pairs.items():
            if 'question' in entry and entry['question'].strip() == target_question:
                predicted_clip = entry.get('clip', '')
                print(f"        Found match in JSON: Predicted clip: {predicted_clip}")
                return {
                    "predicted_clip": predicted_clip,
                    "raw_response": "From JSON cache",
                    "processed_response": "From JSON cache"
                }
        
        print("        No match found in JSON, falling back to API...")
    
    except Exception as e:
        print(f"        JSON loading error: {e}, falling back to API...")
    
    # å¦‚æœJSONä¸­æœªæ‰¾åˆ°ï¼Œæ‰§è¡ŒåŸAPIé€»è¾‘
    # æ„å»ºpromptï¼ˆç®€åŒ–ï¼šå‡è®¾é€‰é¡¹å›ºå®šä¸ºa0-a4ï¼‰
    prompt_content = f"""
Question: {question_data['q']}
Options:
a0: {question_data.get('a0', '')}
a1: {question_data.get('a1', '')}
a2: {question_data.get('a2', '')}
a3: {question_data.get('a3', '')}
a4: {question_data.get('a4', '')}

Subtitles: {sub_block}

The subtitles are formatted as <clip_label>subtitle_content</clip_label>, where each < > pair contains a clip label followed by its corresponding subtitle content.

Based on the question and subtitles, determine:
1. The specific clip label where the answer to this question occurs or is mentioned (output in <clip>label</clip> format)
Please analyze the given question and provide the following information:
<clip>
clip_label (the specific clip where the question's answer can be found in the video)
</clip>
"""
    
    try:
        raw_response = grounding_llm_generate(prompt_content)
        print(f"        Raw response: {raw_response}")
        if raw_response.startswith("Error:"):
            return {"error": raw_response}
        
        processed_response = postprocess_response(raw_response)
        predicted_clip = extract_clip_content(processed_response)
        
        print(f"        Predicted clip: {predicted_clip}")
        
        return {
            "predicted_clip": predicted_clip,
            "raw_response": raw_response,
            "processed_response": processed_response
        }
    except Exception as e:
        print(f"        âŒ Analysis error: {e}")
        return {"error": str(e)}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å•æ ·æœ¬å¤„ç†å‡½æ•°ï¼ˆæ¨¡å—åŒ–æå–ï¼Œä¾¿äºè°ƒè¯•ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def process_single_question(prompt: str, vid: str, question: str, 
                           question_data: Dict, episode_sub_block: str, clip_subtitles: Dict[str, str],
                           max_turn: int = 5) -> Dict[str, Any]:
    """
    å¤„ç†å•ä¸ªé—®é¢˜çš„æ ¸å¿ƒé€»è¾‘ï¼ˆä»å•æ ·æœ¬demoæå–ï¼Œä¾¿äºç‹¬ç«‹è°ƒè¯•æˆ–åœ¨æ‰¹é‡ä¸­è°ƒç”¨ï¼‰ã€‚
    - æ”¯æŒæ–°actionï¼Œæ— æ—¶é—´ã€‚
    - ä½¿ç”¨main_llm_generateï¼ˆAPIè°ƒç”¨æ›¿æ¢æœ¬åœ°æ¨¡å‹ï¼‰ã€‚
    - ç§»é™¤bboxä¾èµ–ï¼Œä»…è§†è§‰æŸ¥è¯¢ã€‚
    - ä¿®æ”¹ï¼šç§»é™¤ground_truthå’Œsub_blockå‚æ•°ï¼Œå› ä¸ºsub_blockå·²åŒ…å«åœ¨promptä¸­ï¼Œä¸”æ¨ç†é˜¶æ®µæ— éœ€ground_truthã€‚
    - æ·»åŠ clip_subtitleså‚æ•°ï¼Œç”¨äºrequest_groundingæ—¶è·å–æ–°subã€‚
    """
    # åˆå§‹åŒ–è®°å½•
    record = {
        "vid": vid, 
        "question": question, 
        "turns": [],
        "final_answer": "",
        "prompt": prompt
    }
    
    # åˆå§‹åŒ–å®Œæ•´å¯¹è¯prompt
    conversation_history = prompt
    
    # åŠ¨æ€å¤šè½®å¯¹è¯å¾ªç¯
    final_answer = ""
    
    for turn in range(max_turn):
        print(f"\n{'='*60}")
        print(f"  Turn {turn + 1}/{max_turn}")
        print(f"{'='*60}")
        
        # å¦‚æœæ˜¯æœ€åä¸€è½®ï¼Œè¿½åŠ å¼ºåˆ¶Action Cçš„æç¤º
        if turn == max_turn - 1:
            conversation_history += (
                "\nThis is the final turn. Please directly perform Action C and "
                "provide the final answer in <answer>...</answer> format.\n"
            )
        # ä½¿ç”¨APIç”Ÿæˆå“åº”ï¼ˆæ›¿æ¢local_llm_generateï¼‰
        raw_response = main_llm_generate(conversation_history,"grok-4-fast-reasoning")
        
        # å“åº”æˆªæ–­å¤„ç†ï¼ˆå¦‚æœæœ‰postprocess_responseï¼Œä¿ç•™ï¼›å¦åˆ™ä½¿ç”¨åŸå§‹ï¼‰
        response = postprocess_response(raw_response) if 'postprocess_response' in globals() else raw_response
        print(f"ğŸ¤– LLM Response:\n{response}")
        
        # ç›´æ¥è§£æactionï¼ˆæ— timeï¼‰
        action_type, content = parse_action_from_response(response)
        print(f"ğŸ·ï¸  Parsed action - Type: {action_type}, Content: {content[:100]}...{'...' if len(content) > 100 else ''}")
        
        # è®°å½•å½“å‰è½®æ¬¡
        turn_record = {
            "turn": turn + 1,
            "response": response,
            "action_type": action_type,
            "content": content,
            "is_done": False
        }
        
        # æ‰§è¡Œactionï¼ˆæ— bboxç‰ˆæœ¬ï¼‰
        print(f"ğŸš€ Executing action: {action_type}")
        result_content, is_done = execute_action(
            action_type, content, vid, text_client=grounding_client, vision_client=vision_client,
            question_data=question_data, episode_sub_block=episode_sub_block, clip_subtitles=clip_subtitles
        )
        
        turn_record["result_content"] = result_content
        turn_record["is_done"] = is_done
        
        print(f"ğŸ“‹ Action result:\n{result_content}")
        
        # å¦‚æœæ˜¯ç­”æ¡ˆï¼Œç»“æŸå¯¹è¯
        if is_done and action_type == 'answer':
            final_answer = content
            print(f"ğŸ‰ Found answer in turn {turn + 1}: {final_answer}")
            record["turns"].append(turn_record)
            break
        
        # å¦‚æœä¸æ˜¯ç­”æ¡ˆï¼Œå°†ç»“æœæ‹¼æ¥åˆ°å¯¹è¯å†å²
        conversation_history += result_content
        print(f"ğŸ”„ Updated conversation history with result content")
        
        # æ›´æ–°vidä¸ºæœ€æ–°predicted_clipï¼ˆå¦‚æœgroundingè¿”å›äº†æ–°clipï¼‰
        if action_type == 'request_grounding' and "<New_clip>" in result_content:
            # æå–predicted_clipï¼ˆå‡è®¾æ ¼å¼ä¸º<New_clip>clip_id + sub</New_clip>ï¼‰
            match = re.search(r"<New_clip>(.*?) \+", result_content, re.DOTALL)
            if match:
                vid = match.group(1).strip()
                print(f"Updated vid to: {vid}")
        
        record["turns"].append(turn_record)
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§è½®æ•°
        if turn == max_turn - 1:
            print(f"  Reached maximum turns ({max_turn})")
    
    # ä¿å­˜ç»“æœ
    record["final_answer"] = final_answer
    record["conversation_history"] = conversation_history
    
    return record

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ–°å¢ï¼šå¤„ç†å•ä¸ªé—®é¢˜çš„åŒ…è£…å‡½æ•°ï¼ˆç”¨äºå¤šçº¿ç¨‹ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def process_question_wrapper(q: Dict, total: int, clip_subtitles: Dict[str, str], max_turn: int) -> Dict[str, Any]:
    """
    åŒ…è£…å‡½æ•°ï¼šå¤„ç†å•ä¸ªé—®é¢˜ï¼Œç”¨äºå¤šçº¿ç¨‹è°ƒç”¨ã€‚
    è¿”å›recordå­—å…¸ã€‚
    """
    try:
        # ä¸ºæ¯ä¸ªé—®é¢˜æ·»åŠ original_idxï¼ˆä¾¿äºè¿½è¸ªï¼‰
        q['original_idx'] = total
        print(f"Processing question {total}")
        
        # å…ˆè¿è¡Œå®šä½é€»è¾‘ï¼šä½¿ç”¨æ•´ä¸ªepisodeçš„sub_blockè¿›è¡Œgrounding
        episode_prefix = q["vid_name"][:6]
        episode_sub_block = build_subtitles_for_episode(clip_subtitles, episode_prefix)
        
        # è°ƒç”¨groundingåˆ†æï¼ˆé»˜è®¤1è½®ï¼Œæ¨ç†é˜¶æ®µæ— GTï¼‰
        grounding_result = analyze_single_question_api(q, episode_sub_block, attempt_round=1)
        
        # è·å–predicted_clipï¼Œå¦‚æœå¤±è´¥åˆ™fallbackåˆ°åŸvid_name
        predicted_clip = grounding_result.get("predicted_clip", q["vid_name"])
        print(f"Predicted clip for question {total}: {predicted_clip}")
        
        # åŸºäºpredicted_clipè·å–æ–°çš„sub_blockï¼ˆä»…è¯¥clipçš„å­—å¹•ï¼‰
        sub_block = get_clip_subtitle(clip_subtitles, predicted_clip)
        
        # æ„å»ºåˆå§‹promptï¼ˆæ–°actionç©ºé—´ï¼Œä½¿ç”¨æ–°çš„sub_blockï¼‰
        initial_prompt = f"""You must follow these rules in every turn:
Reasoning First: Start by conducting your reasoning inside <reasoning>...</reasoning>. This is where you analyze the current information and decide your next step.
Choose One Action: After reasoning, you must choose exactly one of the following three actions:

Action A: If the current information is insufficient or you are somewhat uncertain, and you need to search for visual information on the current located <clipX>, then search for visual information. If your reasoning indicates that you lack necessary visual knowledge, you can call a visual engine. To do this, use the following format: <search>query</search>

Action B: If the current information is insufficient or you are somewhat uncertain, and you cannot obtain the final answer from the previous location and its possible visual information, then you need to call the grounding agent again for relocation, and output in the <request_grounding> format.

Action C: Provide the Final Answer
If your reasoning indicates that you have enough information to answer, provide the final answer inside <answer>...</answer>.
The answer must be concise and direct.

question: {q['q']}
a0: {q['a0']}
a1: {q['a1']}
a2: {q['a2']}
a3: {q['a3']}
a4: {q['a4']}
<information>subtitles: {sub_block}</information>
"""
        
        # å¤„ç†å•ä¸ªé—®é¢˜ï¼ˆä½¿ç”¨predicted_clipä½œä¸ºvidï¼Œç§»é™¤ground_truthå’Œsub_blockï¼Œæ·»åŠ clip_subtitlesï¼‰
        record = process_single_question(
            prompt=initial_prompt,
            vid=predicted_clip,
            question=q['q'],
            question_data=q,
            episode_sub_block=episode_sub_block,
            clip_subtitles=clip_subtitles,
            max_turn=max_turn
        )
        
        # åœ¨recordä¸­æ·»åŠ groundingä¿¡æ¯ï¼ˆæ— GTç›¸å…³ï¼‰
        record["predicted_clip"] = predicted_clip
        
        # ç®€å•ç»Ÿè®¡
        print(f"  Result: {len(record['turns'])} turns, Answer: {record['final_answer']}")

        return record

    except Exception as e:
        print(f"Error processing question {total}: {e}")
        return {}  # è¿”å›ç©ºå­—å…¸è¡¨ç¤ºå¤±è´¥

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ä¸»æµç¨‹ï¼ˆæ‰¹é‡å¤„ç†ç‰ˆï¼Œé›†æˆæ–°actionç©ºé—´ï¼Œæ— æ—¶é—´ä¾èµ–ï¼Œä»…è§†è§‰æŸ¥è¯¢ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_enhanced_pipeline(checkpoint_step: str, max_turn: int = 5, 
                         gpu_memory_utilization: float = 0.4, num_threads: int = 10) -> None:
    """
    å¢å¼ºç‰ˆä¸»æµç¨‹ï¼šæ‰¹é‡å¤„ç†å¤šä¸ªé—®é¢˜ï¼Œæ”¯æŒæ–°actionç©ºé—´ï¼ˆsearch, request_grounding, answerï¼‰ã€‚
    - ç§»é™¤æ—¶é—´èŒƒå›´ä¾èµ–ï¼Œä»…åœ¨search actionæ—¶æ‰§è¡Œè§†è§‰æŸ¥è¯¢ï¼ˆä½¿ç”¨process_and_query_segï¼‰ã€‚
    - request_groundingä½¿ç”¨å ä½ç¬¦å¤„ç†ã€‚
    - ç§»é™¤bboxä¾èµ–ï¼Œä»…è§†è§‰æŸ¥è¯¢ã€‚
    - æå–å•æ ·æœ¬é€»è¾‘åˆ°process_single_questionï¼Œä¾¿äºè°ƒè¯•ã€‚
    - ç®€åŒ–è¾“å‡ºï¼Œä»…ä¿å­˜ä¸€ä¸ªç»“æœæ–‡ä»¶ï¼ˆsummaryï¼‰ã€‚
    - ä¿®æ”¹ï¼šåœ¨å¤„ç†å•ä¸ªé—®é¢˜å‰ï¼Œå…ˆè¿è¡Œå®šä½é€»è¾‘ï¼ˆgrounding_clip.pyé€»è¾‘ï¼‰ï¼Œè·å–predicted_clipä½œä¸ºvidã€‚
    - åŸºäºpredicted_clipè·å–æ–°çš„sub_blockï¼Œç”¨äºinitial_promptã€‚
    - process_single_questionç§»é™¤ground_truthå’Œsub_blockå‚æ•°ã€‚
    - æ¨ç†é˜¶æ®µè°ƒæ•´ï¼šç§»é™¤æ‰€æœ‰GTå¯¹æ¯”å’Œç›¸å…³è®°å½•å­—æ®µï¼Œé˜²æ­¢GTæ³„éœ²ã€‚
    - æ·»åŠ clip_subtitlesä¼ é€’ï¼Œç”¨äºrequest_groundingæ—¶è·å–æ–°subã€‚
    - ç§»é™¤æœ¬åœ°æ¨¡å‹åˆå§‹åŒ–ï¼Œä½¿ç”¨APIå®¢æˆ·ç«¯ã€‚
    - æ–°å¢ï¼šæ”¯æŒå¤šçº¿ç¨‹å¤„ç†ï¼Œé»˜è®¤10ä¸ªçº¿ç¨‹ã€‚
    
    Args:
        checkpoint_step: checkpointæ­¥æ•°
        max_turn: æœ€å¤§å¯¹è¯è½®æ•°
        gpu_memory_utilization: GPUå†…å­˜åˆ©ç”¨ç‡
        num_threads: çº¿ç¨‹æ•°ï¼Œé»˜è®¤10
    """
    # åŠ è½½æ•°æ® - ä¿®æ”¹subs_pathä¸ºclipçº§åˆ«çš„å­—å¹•æ–‡ä»¶ï¼ˆä¸grounding_clip.pyä¸€è‡´ï¼‰
    subs_path       = "../Tvqa_data/all_episodes_subtitles_by_clips.json"
    questions_path  = "../Tvqa_data/tvqa_plus_val.json"
    clip_subtitles  = json.load(open(subs_path,  encoding='utf-8'))
    questions       = json.load(open(questions_path, encoding='utf-8'))


    results: List[Dict[str, Any]] = []
    total = 0
    consecutive_errors = 0
    max_consecutive_errors = 5

    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†é—®é¢˜
    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:
        # æäº¤ä»»åŠ¡ï¼šæ¯ä¸ªé—®é¢˜ä¸€ä¸ªfuture
        futures = []
        for idx, q in enumerate(questions, start=1):
            total = idx
            future = executor.submit(process_question_wrapper, q, total, clip_subtitles, max_turn)
            futures.append(future)

        # æ”¶é›†ç»“æœ
        for future in concurrent.futures.as_completed(futures):
            record = future.result()
            if record:  # å¦‚æœæˆåŠŸ
                results.append(record)
                consecutive_errors = 0
            else:
                consecutive_errors += 1
                if consecutive_errors >= max_consecutive_errors:
                    print(f"Too many consecutive errors ({consecutive_errors}), stopping...")
                    break  # æ³¨æ„ï¼šè¿™ä¸ä¼šåœæ­¢å·²æäº¤çš„ä»»åŠ¡ï¼Œä½†å¯ä»¥æå‰ç»“æŸæ”¶é›†

    print(f"\nTotal processed: {total}")

    # â”€â”€â”€ ä¿å­˜ç®€åŒ–ç»“æœæ–‡ä»¶ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    simplified_results = []
    for result in results:
        simplified_result = {
            "vid": result["vid"],
            "question": result["question"],
            "num_turns": len(result["turns"]),
            "final_answer": result["final_answer"],
            "predicted_clip": result.get("predicted_clip", "")
        }
        simplified_results.append(simplified_result)
    
    # æ–°å¢ï¼šä¿å­˜è¯¦ç»†çš„ conversation_history å’Œæœ€åä¸€è½® LLM output
    detailed_results = []
    for result in results:
        # è·å–æœ€åä¸€è½®çš„ responseï¼ˆå¦‚æœå­˜åœ¨ turnsï¼‰
        last_response = result["turns"][-1]["response"] if result["turns"] else ""
        detailed_result = {
            "vid": result["vid"],
            "question": result["question"],
            "conversation_history": result["conversation_history"],
            "last_llm_response": last_response
        }
        detailed_results.append(detailed_result)
        
    total_vision_calls = sum(sum(1 for t in result["turns"] if t["action_type"] == "search") for result in results)
    total_grounding_calls = sum(sum(1 for t in result["turns"] if t["action_type"] == "request_grounding") for result in results)
    
    # æ–°å¢ï¼šè®¡ç®—å‡†ç¡®ç‡ï¼ˆåœ¨æµ‹è¯•å®Œå…¨éƒ¨é¢˜ç›®ä¹‹åç»Ÿè®¡ï¼‰
    correct_count = 0
    for result, q in zip(results, questions):
        gt_answer = f"a{q['answer_idx']}"  # GT answerï¼Œå¦‚ "a0"
        pred_answer = result["final_answer"].strip().lower()  # é¢„æµ‹ç­”æ¡ˆï¼Œç¡®ä¿å°å†™æ¯”è¾ƒ
        if pred_answer == gt_answer.lower():
            correct_count += 1
    
    accuracy = correct_count / len(questions) if len(questions) > 0 else 0.0
    
    simplified_output = {
        "checkpoint_step": checkpoint_step,
        "model_path": f"../qwen2.5-7b-grpo_step-{checkpoint_step}",
        "gpu_memory_utilization": gpu_memory_utilization,
        "total": total,
        "max_turn": max_turn,
        "metadata": {
            "avg_turns": sum(r["num_turns"] for r in simplified_results) / len(simplified_results) if simplified_results else 0,
            "vision_calls_total": total_vision_calls,
            "grounding_calls_total": total_grounding_calls,
            "completed_questions": len([r for r in simplified_results if r["final_answer"]]),
            "completion_rate": len([r for r in simplified_results if r["final_answer"]]) / len(simplified_results) if simplified_results else 0,
            "accuracy": accuracy  # æ–°å¢å‡†ç¡®ç‡
        },
        "results": simplified_results
    }
    
    simplified_filename = f"./eval_grok-4-fast-reasoning.json"
    with open(simplified_filename, "w", encoding="utf-8") as f:
        json.dump(simplified_output, f, ensure_ascii=False, indent=2)
    
    print(f"Summary results saved to {simplified_filename}")

    # æ–°å¢ï¼šåªä¿å­˜ detailed_results
    detailed_filename = f"./data_log_eval_grok-4-fast-reasoning.json"
    with open(detailed_filename, "w", encoding="utf-8") as f:
        json.dump(detailed_results, f, ensure_ascii=False, indent=2)
    
    # ç»Ÿè®¡ä¿¡æ¯
    if simplified_results:
        metadata = simplified_output["metadata"]
        print(f"\nğŸ“Š Statistics for 7B checkpoint-{checkpoint_step}:")
        print(f"Model path: ../qwen2.5-7b-grpo_step-{checkpoint_step}")
        print(f"GPU memory utilization: {gpu_memory_utilization}")
        print(f"Average turns per question: {metadata['avg_turns']:.2f}")
        print(f"Total vision calls: {metadata['vision_calls_total']}")
        print(f"Total grounding calls: {metadata['grounding_calls_total']}")
        print(f"Vision calls per question: {metadata['vision_calls_total']/len(simplified_results):.2f}")
        print(f"Completed questions: {metadata['completed_questions']}/{len(simplified_results)}")
        print(f"Completion rate: {metadata['completion_rate']:.2%}")
        print(f"Accuracy: {metadata['accuracy']:.2%}")  # æ–°å¢æ‰“å°å‡†ç¡®ç‡
        
        # è½®æ¬¡åˆ†å¸ƒç»Ÿè®¡
        turn_counts = {}
        for r in simplified_results:
            turns = r["num_turns"]
            turn_counts[turns] = turn_counts.get(turns, 0) + 1
        
        print(f"\nğŸ”„ Turn distribution:")
        for turns in sorted(turn_counts.keys()):
            count = turn_counts[turns]
            print(f"  {turns} turns: {count} questions ({count/len(simplified_results)*100:.1f}%)")
# Add this at the end of the file
def main():
    """Main function to run the enhanced pipeline."""
    # Example parameters

    max_turn = 5

    run_enhanced_pipeline("api", max_turn, 0.1, num_threads=30)  

if __name__ == "__main__":
    main()